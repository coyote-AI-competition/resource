# コヨーテAIゲームの戦略

## Abstract

本研究においては、コヨーテAIゲームの戦略を強化学習を用いて実装した。コヨーテAIゲームは、プレイヤーが手札を持ち、他のプレイヤーと対戦するカードゲームである。特に、コヨーテAIゲームにおいては、?（不明なカード）が存在する場合と存在しない場合で戦略が異なるため、それぞれのケースに対して最適な戦略を考察した。
また、DQNベースの強化学習戦略を行うことによって、コヨーテAIゲームにおける最適な行動を学習させた。最終的に強化学習エージェントは、Plus1エージェントに対して圧倒的な勝率を示し、コヨーテゲームにおいて、DQN由来の手法が有効であることを示した。

<!-- 動画埋め込み -->

<video src='yaduya.mp4'  controls="true" ></video>


## コヨーテに大切な制約

コヨーテのゲームの本質は、自分がゲームを終わらせないのであれば、少なくとも+1以上の数を現在の盤面の合計値に加えなければならないという制約が存在する。

また、もう一つの要素としては盤面の中で常にわからない情報は、自分のカードだけであるということがあげられる。つまり、まず初めに、自分のカードがわかりきっている状態では確定ゲームになる。

そこで私は次のようなアプローチでコヨーテの最適戦略を練ることにした。

- 自分のカードが完全にわかった状態での最適戦略を考える。
- ?が存在する条件下での最適戦略を考える。
- その後自分のカードがわからない状態での自分のカードの確率分布から自分のカードを推定する。
- その後、推定したカード情報を元に最適戦略化での最適戦略を考える。
- 最適戦略を行うモデルに対して強化学習を行いブラフを学習させる


## 自分のカードが完全にわかっている場合のコヨーテ（？なしバージョン）

自分のカードが完全にわかっている場合、最適戦略は数学的にとかなくともtotal-1を初めに宣言し、その後つぎの人がtotalをいうしかなくそれによってゲームは終了する。$f(x,x_1,x_2,x_3,x_4,x_5)$がコヨーテカードの出色された全体の集合であると仮定して、次のようにtotalを定義する。
$$
    T = total = f(x_1, x_2, x_3, x_4,x_5 ,x)
$$
ゆえに初手を出すことができる人が確実に勝てるゲームになる。

## ?が存在する時のコヨーテゲーム


?が存在する場合は話が変わってくる。まず初めに?存在下での最適戦略を考えよう。

- ?が存在する場合、自分のカードでも誰でもそれが?であるということを認識することができる。
- この場合？は少なくとも完全にランダムでなければならない。

ゆえに、?を推察することができればこのゲームは勝利することができる。

ここで重要なことは少なくとも?によりもたらされる確率により勝敗が決するということである。
つまり、？が存在する場合の確率分布と期待値を計算することで、勝てなくなる手を打つことがないようにできるはずである。

ここでコヨーテのゲームにおいて重要だと思われることは、次のターンが回ってきた場合には、自分の数の+6を必ずしなければならないということである。
そこで、このゲームの場合のEstimateは全員同じはずであるならば、最初にできるだけ高いestimate-Nを宣言する。逆に、estimate-Nを宣言された場合には、基本的に、そのゲームにおいて勝利する確率が低くなるために、（このゲームにおいては、Estimateの確率分布は、基本的に存在しうる手札の確率分布に従う。そこで、このゲームに置いて、高い数字を宣言することは、相手に対して、コヨーテされる確率を高め、さらにコヨーテされたときに負ける確率が高くなる。）つまり基本的に確率分布に従った方が確立として高いと考えられる。特に複数回行うゲームではその確率は非常に高くなる。
そのために確率的に考えるとこの場合のコヨーテゲームは、確率分布に従って行動することが最適戦略になる。



## ?が複数枚存在する時のコヨーテ

?が複数枚存在する場合にも、まずは基本的にありえる通りを全て計算する必要がある。
そこで?が存在する場合にも非存在の場合にも、基本的にありえる通りを全て計算する必要があると考える。

そこでありうる手を全て計算することにより、その確率分布を計算することができる。
そこで、確率分布から、現在コヨーテしたときに、勝てる確率を計算することができる。
コヨーテを宣言することは基本的に確率的挙動である。

また、この場合のコヨーテについては自分が、?を引く可能性もあるためその可能性も考えておく必要がある。
そのため、この時のコヨーテの宣言タイミングは、固定となる可能性が高い。そこで自分がコヨーテする確率として、確実な場合を除いて勝率が非常に0に近くなったタイミングである。

この場合のルールベースのエージェントの結果としては以下のようになった。
```
100回の試行を行った結果
PreAI1: 10 wins Plus1エージェント
PreAI2: 16 winsPlus1エージェント
PreAI3: 0 wins 定数値エージェント
PreAI4: 11 wins Plus1エージェント
PreAI5: 24 wins Plus1エージェント
PreAI6: 39 wins 自分のモデル
```
圧倒的に、自分のモデルが勝利していることがわかる。ここでいうPlus1エージェントは基本的に論文の再現モデルである。

### 確率を計算することに対する障壁

1. 確率を計算をするために必要な情報が基本的に、実装的に手札のカウンティングができない。
2. 自分のターン終了後に自分の手札を取得することができない。
3. 自分のターンがこない場合には、そのゲームの観測ができない。

など、今回かなりの障壁がある。（先に取得可能な手札について定義していなかったことが悪いが）
しかし全員ほとんど同じ条件である。そこで、カードを自分が知っている手札の情報から推測することにしよう。基本的に対戦相手も同じような条件であり、自分の手番に回らずに来たとしてもその発生確率もそのほかのエージェントと同じであることから少なくとも自分が持っている情報で、推定するほうが推定しないよりもよりよいエージェントであった。（これは実験済み）

したがってこのような確率のもとに、コヨーテの確率を計算することができる。そこでこの特徴量は強化学習にも使えるのではないかと考えた.

## DQNを用いた実装の場合

DQNを用いた実装を行う。この場合には、DQNを用いる。
まず初めに、今回の状況のStateの特徴量として今回は以下のような特徴量を用いることにする。

- 自分の勝ち確率（現在コヨーテしたときに勝てる確率）
- 手札から推察される期待値
- 現在の宣言値
- 現在の残り人数

これらの特徴量を使うことによってかなりロバストに評価することが出来るのではないかと考えた。
また、このような特徴量を用いることにより、よりエージェント相手に左右されない戦略的なコヨーテをすることが出来るのではないかと考えた。

さてここで、DQNは次のような層を設けた。シグモイド関数を用いて、出力を0から1の範囲に制限するようにした。

```
class Net(torch.nn.Module):
    def __init__(self, state_size: int, action_size: int):
        super(Net, self).__init__()
        self.fc1 = torch.nn.Linear(state_size, 64)
        self.fc2 = torch.nn.Linear(64, 64)
        self.fc3 = torch.nn.Linear(64, action_size)

    def forward(self, x):
        x = torch.sigmoid(self.fc1(x))
        x = torch.relu(self.fc2(x))
        # シグモイド関数を使用して出力を0から1の範囲に制限
        x = torch.sigmoid(self.fc3(x))
        return x
```
この時に報酬系として、Rewardを設定する必要がある。
そこで最初期は次のように報酬系を設定することにした。

- コヨーテされなければ+1 コヨーテされたら-1
- 自分がコヨーテして勝てるならば+1、コヨーテして負けるならば -1

のように報酬系を設定した。
このときにPlusOneのエージェントに対しては、1000回の試行を行った際には、
```
PreAI1: 73 wins　Plus1エージェント
PreAI2: 68 wins　Plus1エージェント
PreAI3: 73 wins　Plus1エージェント
PreAI4: 122 wins　Plus1エージェント
PreAI5: 195 wins　Plus1エージェント
PreAI6: 469 wins　自分の強化学習モデル

```
のようになった。


しかし、この場合の報酬系では、基本的に+1の選択をすることが最適解になるために、どのようなエージェントを作成しても基本的に、+1を選択するようなエージェントになる。そこで、これを回避するために、次のような報酬系を設定することにした。

コヨーテして勝った場合には+10,コヨーテして負ければ、-1のように報酬を設定した。コヨーテされない場合は、1,コヨーテされる場合は-1のように報酬を設定した。

このようにすることにより、勝率が0に近い場合にはコヨーテを選択する確率が高くなると考えられる。

```
PreAI1: 125 wins 
PreAI2: 168 wins
PreAI3: 166 wins 
PreAI4: 171 wins 
PreAI5: 256 wins 
PreAI6: 114 wins (自己強化学習エージェント)
```




## 初期学習の方法

学習方法として、ランダムに行動を選択するようにした。そして、次のように設定した。

- プラス1 初期型エージェント3体
- 強化学習エージェント3体

でおこない、その中から勝利数の多いエージェントを選択するようにした。

そしてその後勝利数の多いエージェントの重みを参考にして次のエージェントを選択するようにした。

これにより勝率が上がれば上がるほど学習の回数が多くなり、その強化学習エージェントはより強くなると考えられる。



## 学習の方法

学習としては、PlusOneエージェントに対して、強化学習を行うことにした。また、もう一つコヨーテに確実に勝てるタイミングでコヨーテを選択するエージェントモデルに対してこの二つのアンサンブルをとって、3,2,1のようにエージェントの構成を組んだ。

## 提出物のコードについて

提出物のコードは以上のような過程を踏まえて、ニューラルネットワークをアテンションを用いて実装した。
そして 7000回の学習を行ったモデルを用いて今回のAI大会のコードとする。
```
PreAI1: 4 wins
PreAI2: 9 wins
PreAI3: 12 wins
PreAI4: 8 wins
PreAI5: 35 wins
PreAI6: 32 wins
```




## まとめ

今回提案した手法は、ルールベースのロジックの上にDQNという強化学習を用いた手法を組み合わせるまったく新しい手法を提案する。
そして、この手法は、コヨーテゲームにおいて、PlusOneエージェントに対してかなりの勝率を示すことが出来た。
よって本手法は、コヨーテゲームにおいて、強化学習を用いた手法としてかなり勝率の高い手法であると考えられる。