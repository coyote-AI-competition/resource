import pyspiel
import numpy as np
import coyote
from open_spiel.python.algorithms import external_sampling_mccfr
from open_spiel.python.algorithms import get_all_states
from open_spiel.python.algorithms import deep_cfr
import random
import pickle
import time
import tensorflow
import os
import tqdm
import matplotlib.pyplot as plt
import numpy as np

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()  # TF1äº’æ›ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ


_NUM_PLAYERS = 6  # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
_NUM_DECLARATIONS = 121  # å®£è¨€ã®æœ€å¤§å€¤

_GAME_TYPE = pyspiel.GameType(
    short_name="python_coyote",
    long_name="Python Coyote",
    dynamics=pyspiel.GameType.Dynamics.SEQUENTIAL,
    chance_mode=pyspiel.GameType.ChanceMode.EXPLICIT_STOCHASTIC,
    information=pyspiel.GameType.Information.IMPERFECT_INFORMATION,
    utility=pyspiel.GameType.Utility.ZERO_SUM,
    reward_model=pyspiel.GameType.RewardModel.TERMINAL,
    max_num_players=6,
    min_num_players=2,
    provides_information_state_string=True,
    provides_information_state_tensor=True,
    provides_observation_string=True,
    provides_observation_tensor=False,
    parameter_specification={},  # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã ãŒæ˜ç¤º
    default_loadable=False,      # è‡ªä½œPythonã‚²ãƒ¼ãƒ ã¯Falseã«ã—ã¦ãŠãã¨å®‰å¿ƒ
    provides_factored_observation_string=False
)


_GAME_INFO = pyspiel.GameInfo(
    num_distinct_actions=2,  # å®£è¨€ + ãƒãƒ£ãƒ¬ãƒ³ã‚¸
    max_chance_outcomes=0,
    num_players=_NUM_PLAYERS,
    min_utility=-1.0,
    max_utility=1.0,
    utility_sum=0.0,
    max_game_length=100,
)


class CoyoteState(pyspiel.State):
    def __init__(self, game):
        super().__init__(game)
        self._cur_player = 0
        self._is_terminal = False
        self._player_lives = [1] * _NUM_PLAYERS
        self._player_active = [True] * _NUM_PLAYERS
        self.current_declaration = 0
        self.last_declarer = None
        rnd = random.Random(time.time_ns())
        self.deck = coyote.game.Deck() # ä½¿ç”¨ãƒ‡ãƒƒã‚­
        self.deck.shuffle() # ãƒ‡ãƒƒã‚­ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        self._cards = [self.deck.draw() for _ in range(_NUM_PLAYERS)]  # å„ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«1æšãšã¤ã‚«ãƒ¼ãƒ‰ã‚’é…ã‚‹
        self.is_double_card = False
        self.is_shuffle_card = False
        self._used_cards = []  # ä½¿ç”¨æ¸ˆã¿ã‚«ãƒ¼ãƒ‰
        self._all_cards = [-10, -5, -5, 0, 0, 0,
                      1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3,
                      4, 4, 4, 4, 5, 5, 5, 5, 
                      10, 10, 10, 15, 15, 20, 
                      100, 101, 102, 103]
        # print("Game started!")


    def calc_card_sum(self, cards):
        return coyote.game.calc_card_sum(self, cards)

    def current_player(self):
        return pyspiel.PlayerId.TERMINAL if self._is_terminal else self._cur_player

    def _legal_actions(self, player):
        # if not self._player_active[player]:
        #     return []
        # if self.last_declarer is None: 
        #     return [0]  # æœ€åˆã®ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯å®£è¨€ã®ã¿
        # if self.current_declaration >= _NUM_DECLARATIONS - 1:
        #     return [1]
        actions = [0, 1] # 0: å®£è¨€, 1: ãƒãƒ£ãƒ¬ãƒ³ã‚¸
        return actions
    
    def _apply_action(self, action):
        if (action == 1 or self.current_declaration >= _NUM_DECLARATIONS - 1) and self.last_declarer is not None:  # ãƒãƒ£ãƒ¬ãƒ³ã‚¸
            # print(f"Current Cards: {self._cards}")
            # print(f"Player {self._cur_player} challenges with declaration {self.current_declaration}.")
            true_total = coyote.game.convert_card(self, self._cards, False, self.deck)

            # used_cardsã«ã‚«ãƒ¼ãƒ‰ã‚’è¿½åŠ 
            for i in range(_NUM_PLAYERS):
                if self._player_active[i]:
                    # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªå ´åˆã¯ã‚«ãƒ¼ãƒ‰ã‚’è¿½åŠ 
                    if self._cards[i] in self._all_cards:
                        self._used_cards.append(self._cards[i])

            if self.current_declaration > true_total:
                loser = self.last_declarer
            else:
                loser = self._cur_player
            self._player_lives[loser] -= 1
            if self._player_lives[loser] <= 0:
                self._player_active[loser] = False
            self._is_terminal = sum(self._player_active) <= 1
            self.current_declaration = 0

            if self.is_shuffle_card:
                # SHUFFLEã‚«ãƒ¼ãƒ‰ã‚’å¼•ã„ãŸ => å±±æœ­ã‚’ãƒªã‚»ãƒƒãƒˆ
                self.deck.reset()
                self._used_cards = []
                self.is_shuffle_card = False
            for i in range(_NUM_PLAYERS):
                if self._player_active[i]:
                    if len(self.deck.cards) == 0:
                        self.deck.reset()
                        self._used_cards = []
                    # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªå ´åˆã¯ã‚«ãƒ¼ãƒ‰ã‚’å¼•ã
                    card = self.deck.draw()
                    self._cards[i] = card
                else:
                    # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã§ãªã„å ´åˆã¯ã‚«ãƒ¼ãƒ‰ã‚’0ã«ã™ã‚‹
                    self._cards[i] = 0
            # used_cardsã«ã‚«ãƒ¼ãƒ‰ã‚’è¿½åŠ 
            # print(f"New cards: {self._cards}")
            # print(f"Used cards: {self._used_cards}")
            # print(f"Player {loser} loses a life!")
            # print(f"Player lives: {self._player_lives}")
        else:
            # print(f"Player {self._cur_player} declares {self.current_declaration+1}.")
            self.current_declaration = self.current_declaration + 1
            self.last_declarer = self._cur_player
        self._cur_player = (self._cur_player + 1) % _NUM_PLAYERS
        while not self._player_active[self._cur_player]:
            self._cur_player = (self._cur_player + 1) % _NUM_PLAYERS

    def is_terminal(self):
        return self._is_terminal

    def returns(self):
        survive = sum(self._player_active)
        points = _NUM_PLAYERS - survive
        return [points/survive if self._player_active[i] else -1 for i in range(_NUM_PLAYERS)]
    
    def information_state_string(self, player=None):
        if player is None:
            player = self._cur_player

        # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æƒ…å ±çŠ¶æ…‹ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        # å®£è¨€å€¤ 1 + ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒ©ã‚¤ãƒ• _NUM_PLAYERS + ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ‰‹æœ­ (_NUM_PLAYERS - 1) * 4 + remaining_cards 15
        # å®£è¨€å€¤ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        declaration_str = str(self.current_declaration)
        # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒ©ã‚¤ãƒ•ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        lives_str = ",".join([str(self._player_lives[(i+player) % _NUM_PLAYERS]) for i in range(_NUM_PLAYERS)])
        # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ‰‹æœ­ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        hand_str = ",".join([str(self._cards[(i+player+1) % _NUM_PLAYERS]) for i in range(_NUM_PLAYERS-1)])
        # æ®‹ã‚Šã®ã‚«ãƒ¼ãƒ‰ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        cards_kind = [-10, -5, 0, 1, 2, 3, 4, 5, 10, 15, 20, 100, 101, 102, 103]
        visible_cards = self._all_cards.copy()
        for card in self._used_cards:
            if card in visible_cards:
                visible_cards.remove(card)
        for card in [self._cards[i] for i in range(len(self._cards)) if i != player and self._player_active[i]]:
            if card in visible_cards:
                visible_cards.remove(card)
        remaining_cards_str = ",".join([str(np.sum([1 for card in visible_cards if card == cards_kind[i]])) for i in range(15)])
        # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æƒ…å ±çŠ¶æ…‹ã‚’æ–‡å­—åˆ—ã«çµåˆ
        info_state_str = f"{declaration_str},{lives_str},{hand_str},{remaining_cards_str}"
        return info_state_str

    def information_state_tensor(self, player=None):
        if player is None:
            player = self._cur_player
        # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æƒ…å ±çŠ¶æ…‹ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        # å®£è¨€å€¤ 1 + ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒ©ã‚¤ãƒ• _NUM_PLAYERS + ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ‰‹æœ­ (_NUM_PLAYERS - 1) * 4 + remaining_cards 15
        tensor = np.zeros(1 + _NUM_PLAYERS + (_NUM_PLAYERS - 1)*4 + 15, dtype=np.float32)

        # ç¾åœ¨ã®å®£è¨€å€¤ã‚’å…¥ã‚Œã‚‹
        tensor[0] = self.current_declaration

        # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒ©ã‚¤ãƒ•ã‚’å…¥ã‚Œã‚‹
        for i in range(_NUM_PLAYERS):
            tensor[i + 1] = self._player_lives[(i+player) % _NUM_PLAYERS]

        # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ‰‹æœ­ã‚’å…¥ã‚Œã‚‹
        for i in range(_NUM_PLAYERS-1):
            # (value,is_max,is_double,is_question)ã®å½¢ã§å„ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼å…¥ã‚Œã¦ã„ã
            if self._cards[(i+player+1) % _NUM_PLAYERS] == 102:
                tensor[1 + _NUM_PLAYERS + i*4] = 0
                tensor[1 + _NUM_PLAYERS + i*4 + 1] = 1
                tensor[1 + _NUM_PLAYERS + i*4 + 2] = 0
                tensor[1 + _NUM_PLAYERS + i*4 + 3] = 0
            elif self._cards[(i+player+1) % _NUM_PLAYERS] == 100:
                tensor[1 + _NUM_PLAYERS + i*4] = 0
                tensor[1 + _NUM_PLAYERS + i*4 + 1] = 0
                tensor[1 + _NUM_PLAYERS + i*4 + 2] = 1
                tensor[1 + _NUM_PLAYERS + i*4 + 3] = 0
            elif self._cards[(i+player+1) % _NUM_PLAYERS] == 103:
                tensor[1 + _NUM_PLAYERS + i*4] = 0
                tensor[1 + _NUM_PLAYERS + i*4 + 1] = 0
                tensor[1 + _NUM_PLAYERS + i*4 + 2] = 0
                tensor[1 + _NUM_PLAYERS + i*4 + 3] = 1
            elif self._cards[(i+player+1) % _NUM_PLAYERS] == 101:
                tensor[1 + _NUM_PLAYERS + i*4] = 0
                tensor[1 + _NUM_PLAYERS + i*4 + 1] = 0
                tensor[1 + _NUM_PLAYERS + i*4 + 2] = 0
                tensor[1 + _NUM_PLAYERS + i*4 + 3] = 0
            else:
                tensor[1 + _NUM_PLAYERS + i*4] = self._cards[(i+player+1) % _NUM_PLAYERS]
                tensor[1 + _NUM_PLAYERS + i*4 + 1] = 0
                tensor[1 + _NUM_PLAYERS + i*4 + 2] = 0
                tensor[1 + _NUM_PLAYERS + i*4 + 3] = 0

        cards_kind = [-10, -5, 0, 1, 2, 3, 4, 5, 10, 15, 20, 100, 101, 102, 103]

        # æ®‹ã‚Šã®ã‚«ãƒ¼ãƒ‰ã‚’å…¥ã‚Œã‚‹
        visible_cards = self._all_cards.copy()
        for card in self._used_cards:
            if card in visible_cards:
                visible_cards.remove(card)
        for card in [self._cards[i] for i in range(len(self._cards)) if i != player and self._player_active[i]]:
            if card in visible_cards:
                visible_cards.remove(card)

        # æ®‹ã‚Šã®ã‚«ãƒ¼ãƒ‰ã‚’å…¥ã‚Œã‚‹
        for i in range(15):
            tensor[1 + _NUM_PLAYERS + (_NUM_PLAYERS - 1)*4 + i] = np.sum([1 for card in visible_cards if card == cards_kind[i]])
        return tensor


    def __str__(self):
        return f"Cards: {self._cards}\nCurrent declaration: {self.current_declaration}\nLives: {self._player_lives}\nActive players: {self._player_active}"


class CoyoteObserver:
    def __init__(self, params):
        self.params = params or {}

    def set_from(self, state: CoyoteState, player: int):
        visible = [str(card) if i != player else "?" for i, card in enumerate(state._cards)]
        self.obs_str = f"Visible cards: {visible}\nDeclaration: {state.current_declaration}"

    def string_from(self, state: CoyoteState, player: int):
        self.set_from(state, player)
        return self.obs_str


class CoyoteGame(pyspiel.Game):
    def __init__(self, params=None):
        super().__init__(_GAME_TYPE, _GAME_INFO, params or {})
    
    def new_initial_state(self):
        return CoyoteState(self)
    
    def make_py_observer(self, iig_obs_type=None, params=None):
        return CoyoteObserver(params)


# ç™»éŒ²
pyspiel.register_game(_GAME_TYPE, CoyoteGame)


if __name__ == "__main__":

    # cfrã§æ•°ç†ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    # Check for TensorFlow GPU access

    game = CoyoteGame() # ã‚²ãƒ¼ãƒ ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ

    dirname = os.path.dirname(__file__)

    # while True:
    # ä¿å­˜å…ˆãƒ‘ã‚¹
    ckpt_path = os.path.join(dirname, "checkpoints", "deep_cfr.ckpt")
    # for _ in tqdm.tqdm(range(100000)):
    #     # ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹
    with tf.Session() as sess:

        # DeepCFRSolver æ§‹ç¯‰
        solver = deep_cfr.DeepCFRSolver(
            game=game,
            session=sess,
            policy_network_layers=[64, 32, 16, 8],
            advantage_network_layers=[64, 32, 16, 8],
            num_iterations=1,
            num_traversals=5,
            learning_rate=0.01,
            batch_size_advantage=32,
            batch_size_strategy=32,
            memory_capacity=10000
        )

        # å¤‰æ•°åˆæœŸåŒ–
        sess.run(tf.global_variables_initializer())

        # saver æº–å‚™ï¼ˆsolverå¾Œã˜ã‚ƒãªã„ã¨å¤‰æ•°ãŒæ§‹ç¯‰ã•ã‚Œã¦ãªã„ï¼‰
        saver = tf.train.Saver()

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒå­˜åœ¨ã™ã‚Œã°å¾©å…ƒ
        if os.path.exists(ckpt_path + ".index"):
            print("âœ… Checkpoint found. Restoring model...")
            saver.restore(sess, ckpt_path)
            print("âœ… Model restored!")
            # epochã®å¾©å…ƒ
            with open(os.path.join(dirname, "epoch.txt"), "r") as f:
                epoch = int(f.read())
            print("Epoch:", epoch)

            # Advantage lossesã®å¾©å…ƒ
            with open(os.path.join(dirname, "advantage_losses.npy"), "rb") as f:
                advantage_losses_history = np.load(f)
            # print("Advantage losses:", advantage_losses_history[-1])

            # Policy lossã®å¾©å…ƒ
            with open(os.path.join(dirname, "policy_loss.npy"), "rb") as f:
                policy_loss_history = np.load(f)
            print("Policy loss:", policy_loss_history[-1])
        
        else:

            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒå­˜åœ¨ã—ãªã„å ´åˆã¯æ–°è¦ä½œæˆ
            policy_loss_history = np.array([])
            advantage_losses_history = None


        # å­¦ç¿’ï¼ˆè¿½åŠ è¨“ç·´ï¼‰
        print("ğŸš€ Starting DeepCFR training...")
        # é€²æ—è¡¨ç¤º
        for i in tqdm.tqdm(range(100000)):
            # 1ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å­¦ç¿’
            _, advantage_losses, policy_loss = solver.solve()

            # ä¿å­˜
            saver.save(sess, ckpt_path)
            print("ğŸ’¾ Model saved to:", ckpt_path)

            with open(os.path.join(dirname, "epoch.txt"), "w") as f:
                f.write(str(i + 1))

            advantage_losses_ls = np.array([advantage_losses[i] for i in range(_NUM_PLAYERS)]).reshape(-1, _NUM_PLAYERS)

            if advantage_losses_history is None:
                advantage_losses_history = advantage_losses_ls
            else:
                advantage_losses_history = np.vstack((advantage_losses_history, advantage_losses_ls))

            # historyã«è¿½åŠ 
            # advantage_losses_history = np.append(advantage_losses_history, advantage_losses)
            policy_loss_history = np.append(policy_loss_history, policy_loss)

            # ä¿å­˜
            np.save(os.path.join(dirname, "advantage_losses.npy"), advantage_losses_history)
            np.save(os.path.join(dirname, "policy_loss.npy"), policy_loss_history)

            # plot loss
            for i in range(_NUM_PLAYERS):
                plt.plot(advantage_losses_history[:,i], label=f"Player_{i}_Advantage_Loss")
            plt.xlabel("Iteration")
            plt.ylabel("Loss")
            plt.yscale('log')
            plt.title("DeepCFR Advantage Loss")
            plt.legend()
            plt.savefig(os.path.join(dirname, "advantage_loss.png"))
            plt.close()
            
            # plot policy loss
            plt.plot(policy_loss_history, label="Policy Loss")
            plt.xlabel("Iteration")
            plt.yscale('log')
            plt.ylabel("Loss")
            plt.title("DeepCFR Loss")
            plt.legend()
            plt.savefig(os.path.join(dirname, "policy_loss.png"))
            plt.close()

        print("âœ… Training complete!")