import pyspiel
import numpy as np
from open_spiel.python.algorithms import external_sampling_mccfr
from open_spiel.python.algorithms import get_all_states
from open_spiel.python.algorithms import deep_cfr
import random
import pickle
import time
import tensorflow
import os
import tqdm
import matplotlib.pyplot as plt
import numpy as np
from pyspiel import PlayerId

import tensorflow.compat.v1 as tf
from game_setting import *


_NUM_PLAYERS = 6  # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°

if __name__ == "__main__":

    # cfrã§æ•°ç†ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    # Check for TensorFlow GPU access

    game = CoyoteGame() # ã‚²ãƒ¼ãƒ ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ

    dirname = os.path.dirname(__file__)

    # while True:
    # ä¿å­˜å…ˆãƒ‘ã‚¹
    ckpt_path = os.path.join(dirname, f"checkpoints_{_NUM_PLAYERS}", "deep_cfr.ckpt")
    # for _ in tqdm.tqdm(range(100000)):
    #     # ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹
    with tf.Session() as sess:

        # DeepCFRSolver æ§‹ç¯‰
        solver = deep_cfr.DeepCFRSolver(
            game=game,
            session=sess,
            policy_network_layers=[256, 256],
            advantage_network_layers=[256, 256],
            num_iterations=5,
            num_traversals=1000,
            learning_rate=1e-4,
            batch_size_advantage=32,
            batch_size_strategy=32,
            memory_capacity=1e6
        )

        # å¤‰æ•°åˆæœŸåŒ–
        sess.run(tf.global_variables_initializer())

        # saver æº–å‚™ï¼ˆsolverå¾Œã˜ã‚ƒãªã„ã¨å¤‰æ•°ãŒæ§‹ç¯‰ã•ã‚Œã¦ãªã„ï¼‰
        saver = tf.train.Saver()

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒå­˜åœ¨ã™ã‚Œã°å¾©å…ƒ
        if os.path.exists(ckpt_path + ".index"):
            print("âœ… Checkpoint found. Restoring model...")
            saver.restore(sess, ckpt_path)
            print("âœ… Model restored!")
            # epochã®å¾©å…ƒ
            with open(os.path.join(dirname, f"epoch_{_NUM_PLAYERS}.txt"), "r") as f:
                epoch = int(f.read())
            print("Epoch:", epoch)

            # Advantage lossesã®å¾©å…ƒ
            with open(os.path.join(dirname, f"advantage_losses_{_NUM_PLAYERS}.npy"), "rb") as f:
                advantage_losses_history = np.load(f)
            # print("Advantage losses:", advantage_losses_history[-1])

            # Policy lossã®å¾©å…ƒ
            with open(os.path.join(dirname, f"policy_loss_{_NUM_PLAYERS}.npy"), "rb") as f:
                policy_loss_history = np.load(f)
            print("Policy loss:", policy_loss_history[-1])
        
        else:

            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒå­˜åœ¨ã—ãªã„å ´åˆã¯æ–°è¦ä½œæˆ
            policy_loss_history = np.array([])
            advantage_losses_history = None


        # å­¦ç¿’ï¼ˆè¿½åŠ è¨“ç·´ï¼‰
        print("ğŸš€ Starting DeepCFR training...")
        # é€²æ—è¡¨ç¤º
        for i in tqdm.tqdm(range(100000)):
            # 1ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å­¦ç¿’
            _, advantage_losses, policy_loss = solver.solve()

            # ä¿å­˜
            saver.save(sess, ckpt_path)
            print("ğŸ’¾ Model saved to:", ckpt_path)

            with open(os.path.join(dirname, f"epoch_{_NUM_PLAYERS}.txt"), "w") as f:
                f.write(str(i + 1))

            advantage_losses_ls = np.array([advantage_losses[i] for i in range(_NUM_PLAYERS)]).reshape(-1, _NUM_PLAYERS)

            if advantage_losses_history is None:
                advantage_losses_history = advantage_losses_ls
            else:
                advantage_losses_history = np.vstack((advantage_losses_history, advantage_losses_ls))

            # historyã«è¿½åŠ 
            # advantage_losses_history = np.append(advantage_losses_history, advantage_losses)
            policy_loss_history = np.append(policy_loss_history, policy_loss)

            # ä¿å­˜
            np.save(os.path.join(dirname, f"advantage_losses_{_NUM_PLAYERS}.npy"), advantage_losses_history)
            np.save(os.path.join(dirname, f"policy_loss_{_NUM_PLAYERS}.npy"), policy_loss_history)

            # plot loss
            for i in range(_NUM_PLAYERS):
                plt.plot(advantage_losses_history[:,i], label=f"Player_{i}_Advantage_Loss")
            plt.xlabel("Iteration")
            plt.ylabel("Loss")
            plt.yscale('log')
            plt.title("DeepCFR Advantage Loss")
            plt.legend()
            plt.savefig(os.path.join(dirname, f"advantage_loss_{_NUM_PLAYERS}.png"))
            plt.close()
            
            # plot policy loss
            plt.plot(policy_loss_history, label="Policy Loss")
            plt.xlabel("Iteration")
            plt.yscale('log')
            plt.ylabel("Loss")
            plt.title("DeepCFR Loss")
            plt.legend()
            plt.savefig(os.path.join(dirname, f"policy_loss_{_NUM_PLAYERS}.png"))
            plt.close()

        print("âœ… Training complete!")